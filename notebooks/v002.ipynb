{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "v002.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsC5mDuQcnEo",
        "colab_type": "text"
      },
      "source": [
        "# A2C"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxNoyJriQLMd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Conv2D, GlobalAveragePooling2D\n",
        "\n",
        "class ActionValueModel(Model):\n",
        "\n",
        "    def __init__(self, num_actions):\n",
        "\n",
        "        super().__init__('mlp_policy')\n",
        "\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "        # TODO Defining learnable operators\n",
        "\n",
        "        # Actor - Defining operators for policy network\n",
        "        self.feat_extr_0 = Conv2D(filters=32, kernel_size=(3,3), strides=(2,2))\n",
        "        self.feat_extr_1 = Conv2D(filters=32, kernel_size=(3,3), strides=(2,2))\n",
        "        self.feat_extr_2 = Conv2D(filters=32, kernel_size=(3,3), strides=(2,2))\n",
        "        self.feat_extr_3 = Conv2D(filters=32, kernel_size=(3,3), strides=(2,2))\n",
        "        self.ap = GlobalAveragePooling2D()\n",
        "        self.fcl_0 = Dense(128, activation='relu')\n",
        "        self.fcl_1 = Dense(64, activation='relu')\n",
        "        self.fcl_2 = Dense(128, activation='relu')\n",
        "        self.action_picker = Dense(self.num_actions, activation='relu')\n",
        "        #self.logits = Dense(num_actions, name='policy_logits')\n",
        "        #self.dist = ProbabilityDistribution()\n",
        "\n",
        "        # Critic - Defining operators for value network\n",
        "        self.hidden2 = Dense(128, activation='relu')\n",
        "        self.hidden3 = Dense(128, activation='relu')\n",
        "        self.hidden4 = Dense(128, activation='relu')\n",
        "        self.value = Dense(1, name='value')\n",
        "        \n",
        "\n",
        "    def call(self, inputs):\n",
        "        # inputs = [target_array, current_array]\n",
        "\n",
        "        # Converting inputs array into a tensor\n",
        "        input_target, input_current = inputs #x = tf.convert_to_tensor(inputs)\n",
        "\n",
        "        # Encoder -> target\n",
        "        target = self.feat_extr_0(input_target)\n",
        "        target = self.feat_extr_1(target)\n",
        "        target = self.feat_extr_2(target)\n",
        "        target = self.feat_extr_3(target)\n",
        "        latent_target = self.ap(target)\n",
        "\n",
        "        # Encoder -> current\n",
        "        current = self.feat_extr_0(input_current)\n",
        "        current = self.feat_extr_1(current)\n",
        "        current = self.feat_extr_2(current)\n",
        "        current = self.feat_extr_3(current)\n",
        "        latent_current = self.ap(current)\n",
        "\n",
        "        # Producing output\n",
        "        aggr = Concatenate()([latent_target, latent_current])\n",
        "        aggr = self.fcl_0(aggr)\n",
        "        aggr = self.fcl_1(aggr)\n",
        "        aggr = self.fcl_2(aggr)\n",
        "        actions = self.action_picker(aggr)\n",
        "\n",
        "        # Calculating value\n",
        "        hidden_vals = self.hidden2(aggr)\n",
        "        hidden_vals = self.hidden3(hidden_vals)\n",
        "        hidden_vals = self.hidden4(hidden_vals)\n",
        "        value = self.value(hidden_vals)\n",
        "\n",
        "        # returning action actions and value\n",
        "        return actions, value\n",
        "\n",
        "\n",
        "    def action_value(self, obs, target):\n",
        "\n",
        "        # Predicting action and value from observed state\n",
        "        action, value = self.predict([obs,target])\n",
        "        #action = self.dist.predict(logits)\n",
        "\n",
        "        return np.squeeze(action, axis=-1), np.squeeze(value, axis=-1) # ?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBAVH5oNYLOg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.losses import categorical_crossentropy, SparseCategoricalCrossentropy, mean_squared_error\n",
        "\n",
        "class A2CAgent(object) :\n",
        "\n",
        "    def __init__(self, model):\n",
        "\n",
        "        # Hyperparameters settings\n",
        "        self.params = {\"gamma\"               :   0.99, # discount factor\n",
        "                       \"value_loss_factor\"   :    0.5,\n",
        "                       \"entropy_loss_factor\" : 0.0001, # exploration factor\n",
        "                       }\n",
        "\n",
        "        # Defining and compiling model\n",
        "        self.model = model\n",
        "        self.model.compile(optimizer = RMSprop(lr=0.0007),\n",
        "                           loss = [self._logits_loss, \n",
        "                                   self._value_loss,\n",
        "                                   ]\n",
        "                           )\n",
        "    \n",
        "\n",
        "    def train(self, environment, batch_size=32, updates=1000):\n",
        "\n",
        "        # storage helpers for a single batch of data\n",
        "        actions = np.empty((batch_size,), dtype=np.int32)\n",
        "        rewards, dones, values = np.empty((3, batch_size))\n",
        "        observations = np.empty((batch_size,) + environment.observation_space.shape)\n",
        "        \n",
        "        #memory leak tracker\n",
        "        #tr = tracker.SummaryTracker()\n",
        "\n",
        "        # Training loop\n",
        "        episode_rewards = [0.0] # cumulative episode reward (episode_rewards[i] contains the sum of rewards of episode i)\n",
        "        next_obs = environment.reset()\n",
        "        for update in range(updates):\n",
        "            logging.info(\"\\n\\n    UPDATE: %03d \\n\\n\" % (update))\n",
        "            # Collecting batch of training data\n",
        "            for step in range(batch_size):\n",
        "\n",
        "                # Percepting the environment\n",
        "                observations[step] = next_obs.copy()\n",
        "\n",
        "                # Getting action and value predicted for this perception\n",
        "                actions[step], values[step] = self.model.action_value(next_obs[None, :])\n",
        "\n",
        "                # Updating the environment with action\n",
        "                next_obs, rewards[step], dones[step], _ = environment.step(actions[step])\n",
        "\n",
        "                # Updating current episode total reward\n",
        "                episode_rewards[-1] += rewards[step]\n",
        "\n",
        "                # If the episode has come to an end, restarting the environment and initializing cumulative reward for new episode\n",
        "                if dones[step]:\n",
        "                    episode_rewards.append(0.0)\n",
        "                    next_obs = environment.reset()\n",
        "\n",
        "                    # Monitoring\n",
        "                    logging.info(\"\\n Episode: %03d, Reward: %03d\" % (len(episode_rewards)-1, episode_rewards[-2]))\n",
        "                    logging.info(\"   actions: %s; rewards: %s; dones: %s; values: %s; observations: %s\" % (actions.shape, rewards.shape, dones.shape, values.shape, observations.shape))\n",
        "                    logging.info(\"   episode_rewards: %s\" % (len(episode_rewards)))\n",
        "                    #tr.print_diff()\n",
        "            # At that point, we have enough information to make a batch of data\n",
        "\n",
        "            # Making batch\n",
        "            _, next_value = self.model.action_value(next_obs[None, :])\n",
        "            returns, advantages = self._returns_advantages(rewards, dones, values, next_value)\n",
        "\n",
        "            # Training on collected batch\n",
        "            acts_and_advs = np.concatenate([actions[:, None], advantages[:, None]], axis=-1) # trick to include multiple arguments in a loss function\n",
        "            losses = self.model.train_on_batch(observations, [acts_and_advs, returns])\n",
        "\n",
        "            # Monitoring\n",
        "            logging.debug(\"[%d/%d] Losses: %s\" % (update+1, updates, losses))\n",
        "        \n",
        "        #tr.print_diff()\n",
        "        return episode_rewards\n",
        "\n",
        "\n",
        "    def decision(self, perception) :\n",
        "        action, value = self.model.action_value(perception[None, :])\n",
        "        return action\n",
        "\n",
        "\n",
        "    def test(self, environment, render=False):\n",
        "        obs, done, episode_reward = environment.reset(), False, 0\n",
        "        while not done:\n",
        "            action, _ = self.model.action_value(obs[None, :])\n",
        "            obs, reward, done, _ = environment.step(action)\n",
        "            episode_reward += reward\n",
        "            if render:\n",
        "                environment.render()\n",
        "        return episode_reward\n",
        "\n",
        "\n",
        "    def _returns_advantages(self, rewards, dones, values, next_value):\n",
        "        # next_value is the bootstrap value estimate of a future state (the critic)\n",
        "        returns = np.append(np.zeros_like(rewards), next_value, axis=-1)\n",
        "\n",
        "        # Calculating returns as discounted sum of future rewards\n",
        "        for t in reversed(range(rewards.shape[0])):\n",
        "            returns[t] = rewards[t] + self.params['gamma'] * returns[t+1] * (1-dones[t])\n",
        "        returns = returns[:-1]\n",
        "\n",
        "        # Calculating advantages\n",
        "        advantages = returns - values\n",
        "\n",
        "        return returns, advantages\n",
        "    \n",
        "\n",
        "    def _value_loss(self, returns, value):\n",
        "        return self.params['value_loss_factor']*mean_squared_error(returns, value) # value loss is typically MSE between value estimates and returns\n",
        "\n",
        "\n",
        "    def _logits_loss(self, acts_and_advs, logits):\n",
        "\n",
        "        # Separating actions and advantages\n",
        "        actions, advantages = tf.split(acts_and_advs, 2, axis=-1)\n",
        "\n",
        "        # Calculating policy loss\n",
        "        # note: we only calculate the loss on the actions we've actually taken\n",
        "        actions = tf.cast(actions, tf.int32)\n",
        "        weighted_sparse_ce = SparseCategoricalCrossentropy(from_logits=True) # from_logits argument ensures transformation into normalized probabilities\n",
        "        policy_loss = weighted_sparse_ce(actions, logits, sample_weight=advantages) # policy loss is defined by policy gradients, weighted by advantages\n",
        "\n",
        "        # Calculating entropy loss\n",
        "        entropy_loss = categorical_crossentropy(logits, logits, from_logits=True) # entropy loss can be calculated via CE over itself\n",
        "        \n",
        "        return policy_loss - self.params['entropy_loss_factor']*entropy_loss # here signs are flipped because optimizer minimizes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwz2ntFGZMGl",
        "colab_type": "text"
      },
      "source": [
        "# Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weVKaPIYZNuj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from painter.environmentInterfaces.LibMyPaintInterface import *\n",
        "\n",
        "environment = LibMyPaintInterface(episode_length=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biShzrovYlO2",
        "colab_type": "text"
      },
      "source": [
        "# Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgXPtoQHYhlk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = ActionValueModel(num_actions=11)\n",
        "agent = A2CAgent(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGHu_ycAaOlx",
        "colab_type": "text"
      },
      "source": [
        "# Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZpYYXAwYoCu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rewards_sum = agent.test(environment)\n",
        "print(agent.test(environment))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ad1_WTySaZ2u",
        "colab_type": "text"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4rYFsSQaaIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "\n",
        "# set to logging.WARNING to disable logs or logging.DEBUG to see losses as well\n",
        "logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "rewards_history = agent.train(environment, updates=1000)\n",
        "\n",
        "print(\"Finished training.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GISthq52ajAw",
        "colab_type": "text"
      },
      "source": [
        "# Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jB0gwFQRakP-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plotCumulativeRewards(rewards_history) :\n",
        "\n",
        "    plt.style.use('seaborn')\n",
        "\n",
        "    plt.plot(rewards_history)\n",
        "\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Total Reward')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "print(rewards_history)\n",
        "plotCumulativeRewards(rewards_history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhK5_K0yasJP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from painter.animators.Animator import *\n",
        "from PIL import Image\n",
        "\n",
        "input_img = Image.open(\"TODO\").asarray()\n",
        "\n",
        "animator = Animator(agent=agent,\n",
        "                    environment_interface=environment,\n",
        "                    objectif=input_img)\n",
        "animator.anime(target=input_img, fps=10)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}